{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Imp Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nlppreprocess import NLP\n",
    "\n",
    "import math\n",
    "import string\n",
    "punct = string.punctuation\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train_K_Tweets.csv')\n",
    "test=pd.read_csv('test_K_Tweets.csv')\n",
    "dataset=pd.concat([train,test])\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0   1     NaN      NaN   \n",
       "1   4     NaN      NaN   \n",
       "2   5     NaN      NaN   \n",
       "3   6     NaN      NaN   \n",
       "4   7     NaN      NaN   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0                                Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n",
       "1                                                               Forest fire near La Ronge Sask. Canada   \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or...   \n",
       "3                                    13,000 people receive #wildfires evacuation orders in California    \n",
       "4             Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n",
       "\n",
       "   target  \n",
       "0     1.0  \n",
       "1     1.0  \n",
       "2     1.0  \n",
       "3     1.0  \n",
       "4     1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data cleaningÂ¶\n",
    "## a). We will start with cleaning basic text noises such as URLS , Email IDS , punctautions etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "train.text=train.text.apply(lambda x: lowercase_text(x))\n",
    "test.text=test.text.apply(lambda x: lowercase_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  our deeds are the reason of this #earthquake may allah forgive us all\n",
       "1                                                                 forest fire near la ronge sask. canada\n",
       "2    all residents asked to 'shelter in place' are being notified by officers. no other evacuation or...\n",
       "3                                      13,000 people receive #wildfires evacuation orders in california \n",
       "4               just got sent this photo from ruby #alaska as smoke from #wildfires pours into a school \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(text):\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    #text = re.sub('#', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.text=train.text.apply(lambda x: remove_noise(x))\n",
    "test.text=test.text.apply(lambda x: remove_noise(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deeds are the reason of this earthquake may allah forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all residents asked to shelter in place are being notified by officers no other evacuation or sh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive wildfires evacuation orders in california</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent this photo from ruby alaska as smoke from wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0   1     NaN      NaN   \n",
       "1   4     NaN      NaN   \n",
       "2   5     NaN      NaN   \n",
       "3   6     NaN      NaN   \n",
       "4   7     NaN      NaN   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0                                 our deeds are the reason of this earthquake may allah forgive us all   \n",
       "1                                                                forest fire near la ronge sask canada   \n",
       "2  all residents asked to shelter in place are being notified by officers no other evacuation or sh...   \n",
       "3                                            people receive wildfires evacuation orders in california    \n",
       "4               just got sent this photo from ruby alaska as smoke from wildfires pours into a school    \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b). Now we will use NLP preprocessing to process our data ! This actually gave me better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = NLP()\n",
    "\n",
    "train['text'] = train['text'].apply(nlp.process)\n",
    "test['text'] = test['text'].apply(nlp.process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                   our deeds are reason earthquake may allah forgive us\n",
       "1                                                                  forest fire near la ronge sask canada\n",
       "2    residents asked shelter in place are being notified by officers no evacuation shelter in place o...\n",
       "3                                               people receive wildfires evacuation orders in california\n",
       "4                            just got sent photo from ruby alaska smoke from wildfires pours into school\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c). Stemming\n",
    "## Now we have to stem our text , will be using SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def stemming(text):\n",
    "    text = [stemmer.stem(word) for word in text.split()]\n",
    "    return ' '.join(text)\n",
    "\n",
    "train['text'] = train['text'].apply(stemming)\n",
    "test['text'] = test['text'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                               our deed are reason earthquak may allah forgiv us\n",
       "1                                                            forest fire near la rong sask canada\n",
       "2    resid ask shelter in place are be notifi by offic no evacu shelter in place order are expect\n",
       "3                                                  peopl receiv wildfir evacu order in california\n",
       "4                        just got sent photo from rubi alaska smoke from wildfir pour into school\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask the dataset in a new dataframe called \"q\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = train.loc[0:999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Using Spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final processing and load tokens in a column as a set of cleaned words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "l = []\n",
    "k = []\n",
    "\n",
    "for j in range(0,len(q.text)):\n",
    "    x=q.text[j]\n",
    "    doc =nlp(x)\n",
    "    for token in doc:\n",
    "        #l.append(token)\n",
    "        if token.lemma_ != \"-PRON-\":\n",
    "            temp = token.lemma_.lower().strip()\n",
    "            #k.append(temp)\n",
    "        else:\n",
    "            temp = token.lower_\n",
    "        k.append(temp)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    cleaned_tokens = []\n",
    "    for token in k:\n",
    "        if token not in stopwords and token not in punct:\n",
    "            cleaned_tokens.append(token)\n",
    "        \n",
    "        #for i in range(0,len(l)):\n",
    "            #if token != l[i]:\n",
    "                #print(token)\n",
    "                #l.append(token)\n",
    "                #sentance_list.append(sent)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7883"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_word_list_set=list(set(cleaned_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3094"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(All_word_list_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_cleaned = pd.DataFrame(All_word_list_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cargo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>annihil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0\n",
       "0  coaster\n",
       "1    drunk\n",
       "2      bob\n",
       "3    cargo\n",
       "4  annihil"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean words of +ve and -ve tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "k = []\n",
    "\n",
    "for j in range(0,len(q.text)):\n",
    "    if q.target[j]==1:\n",
    "        x=q.text[j]\n",
    "        doc =nlp(x)\n",
    "        for token in doc:\n",
    "            #l.append(token)\n",
    "            if token.lemma_ != \"-PRON-\":\n",
    "                temp = token.lemma_.lower().strip()\n",
    "                #k.append(temp)\n",
    "            else:\n",
    "                temp = token.lower_\n",
    "            k.append(temp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        positive_cleaned_tokens = []\n",
    "        for token in k:\n",
    "            if token not in stopwords and token not in punct:\n",
    "                positive_cleaned_tokens.append(token)\n",
    "\n",
    "            #for i in range(0,len(l)):\n",
    "                #if token != l[i]:\n",
    "                    #print(token)\n",
    "                    #l.append(token)\n",
    "                    #sentance_list.append(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "k = []\n",
    "\n",
    "for j in range(0,len(q.text)):\n",
    "    if q.target[j]==0:\n",
    "        x=q.text[j]\n",
    "        doc =nlp(x)\n",
    "        for token in doc:\n",
    "            #l.append(token)\n",
    "            if token.lemma_ != \"-PRON-\":\n",
    "                temp = token.lemma_.lower().strip()\n",
    "                #k.append(temp)\n",
    "            else:\n",
    "                temp = token.lower_\n",
    "            k.append(temp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        negative_cleaned_tokens = []\n",
    "        for token in k:\n",
    "            if token not in stopwords and token not in punct:\n",
    "                negative_cleaned_tokens.append(token)\n",
    "\n",
    "            #for i in range(0,len(l)):\n",
    "                #if token != l[i]:\n",
    "                    #print(token)\n",
    "                    #l.append(token)\n",
    "                    #sentance_list.append(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5183"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(negative_cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2700"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(positive_cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3094"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(All_word_list_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all three cleaned list in a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'ALL WORDS': All_word_list_set,\n",
    "     'POSITIVE SENTI WORDS': positive_cleaned_tokens,\n",
    "    'NEGATIVE SENTI WORDS': negative_cleaned_tokens}\n",
    "df = pd.DataFrame.from_dict(a, orient='index')\n",
    "df = df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALL WORDS</th>\n",
       "      <th>POSITIVE SENTI WORDS</th>\n",
       "      <th>NEGATIVE SENTI WORDS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coaster</td>\n",
       "      <td>deed</td>\n",
       "      <td>man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drunk</td>\n",
       "      <td>reason</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bob</td>\n",
       "      <td>earthquak</td>\n",
       "      <td>fruit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cargo</td>\n",
       "      <td>allah</td>\n",
       "      <td>summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>annihil</td>\n",
       "      <td>forgiv</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ALL WORDS POSITIVE SENTI WORDS NEGATIVE SENTI WORDS\n",
       "0   coaster                 deed                  man\n",
       "1     drunk               reason                 love\n",
       "2       bob            earthquak                fruit\n",
       "3     cargo                allah               summer\n",
       "4   annihil               forgiv                 love"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Because of their differ in row number None value are generated so replace them with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALL WORDS</th>\n",
       "      <th>POSITIVE SENTI WORDS</th>\n",
       "      <th>NEGATIVE SENTI WORDS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3013</th>\n",
       "      <td>stir</td>\n",
       "      <td>0</td>\n",
       "      <td>danryckert</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ALL WORDS POSITIVE SENTI WORDS NEGATIVE SENTI WORDS\n",
       "3013      stir                    0           danryckert"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.fillna(\"0\")\n",
    "#select all rows containing \"foo\"\n",
    "df[df['NEGATIVE SENTI WORDS'].str.contains('danryckert', regex=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the frequency of each word and load them in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Posi_num = []\n",
    "for j in range (0,len(All_word_list_set)):\n",
    "    temp = df[\"ALL WORDS\"][j]\n",
    "    num = (df['POSITIVE SENTI WORDS'] == temp).sum()\n",
    "    Posi_num.append(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Negative_num = []\n",
    "for j in range (0,len(All_word_list_set)):\n",
    "    temp = df[\"ALL WORDS\"][j]\n",
    "    num = (df['NEGATIVE SENTI WORDS'] == temp).sum()\n",
    "    Negative_num.append(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'ALL WORDS': All_word_list_set,\n",
    "    # 'POSITIVE SENTI WORDS': positive_cleaned_tokens,\n",
    "   # 'NEGATIVE SENTI WORDS': negative_cleaned_tokens,\n",
    "    'POSTIVE_NUMBER':Posi_num,\n",
    "    'NEGATIVE_NUMBER':Negative_num}\n",
    "df = pd.DataFrame.from_dict(a, orient='index')\n",
    "df = df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALL WORDS</th>\n",
       "      <th>POSTIVE_NUMBER</th>\n",
       "      <th>NEGATIVE_NUMBER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coaster</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drunk</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bob</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cargo</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>annihil</td>\n",
       "      <td>19</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ALL WORDS POSTIVE_NUMBER NEGATIVE_NUMBER\n",
       "0   coaster              0               2\n",
       "1     drunk              3               0\n",
       "2       bob              0               4\n",
       "3     cargo              1               0\n",
       "4   annihil             19              37"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df[\"POSTIVE_NUMBER\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Number Nikalte hai yeha dono columns ke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_posi = 0\n",
    "for j in range(0,(len(df.POSTIVE_NUMBER))):\n",
    "    temp = df.POSTIVE_NUMBER[j]\n",
    "    sum_posi = sum_posi + temp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_Negi = 0\n",
    "for j in range(0,(len(df.NEGATIVE_NUMBER))):\n",
    "    temp = df.NEGATIVE_NUMBER[j]\n",
    "    sum_Negi = sum_Negi + temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5183"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_Negi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2700"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_posi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3094"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(All_word_list_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now find the porobablity of positive tweets and negative tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## A). Probablity by Using Laplacian Smoothing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### +ve ke liye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum_posi = 0\n",
    "Laplas_list_posi = []\n",
    "for j in range(0,(len(df.POSTIVE_NUMBER))):\n",
    "    temp = df.POSTIVE_NUMBER[j]\n",
    "    #sum_posi = sum_posi + temp\n",
    "    Laplas_divide_posi = (temp+1)/(2700+3094)\n",
    "    Laplas_list_posi.append(Laplas_divide_posi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = {\"Laplas_posi_data\":Laplas_list_posi}\n",
    "lipo = pd.DataFrame(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -Ve ke liye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum_posi = 0\n",
    "Laplas_list_negi = []\n",
    "for j in range(0,(len(df.NEGATIVE_NUMBER))):\n",
    "    temp = df.NEGATIVE_NUMBER[j]\n",
    "    #sum_posi = sum_posi + temp\n",
    "    divide_negi = temp/5183\n",
    "    Laplas_list_negi.append(divide_negi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lipo[\"Laplas_negi_data\"] = Laplas_list_negi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To check the sum of a probablity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For +ve Probablity\n",
    "\n",
    "Laplas_P_TOTAL_SUM = 0\n",
    "for j in range(0,(len(lipo.Laplas_posi_data))):\n",
    "    temp = lipo.Laplas_posi_data[j]\n",
    "    Laplas_P_TOTAL_SUM = Laplas_P_TOTAL_SUM + temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000282"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Laplas_P_TOTAL_SUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For -ve Probablity\n",
    "\n",
    "Laplas_N_TOTAL_SUM = 0\n",
    "for j in range(0,(len(lipo.Laplas_negi_data))):\n",
    "    temp = lipo.Laplas_negi_data[j]\n",
    "    Laplas_N_TOTAL_SUM = Laplas_N_TOTAL_SUM + temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999813"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Laplas_N_TOTAL_SUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SO Sum are Nearly one means its ok, Now we will append all these probablity in dataframe (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Laplas_Prob_of_P_W\"] = Laplas_list_posi\n",
    "df[\"Laplas_Prob_of_N_W\"] = Laplas_list_negi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALL WORDS</th>\n",
       "      <th>POSTIVE_NUMBER</th>\n",
       "      <th>NEGATIVE_NUMBER</th>\n",
       "      <th>Laplas_Prob_of_P_W</th>\n",
       "      <th>Laplas_Prob_of_N_W</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coaster</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drunk</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bob</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cargo</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>annihil</td>\n",
       "      <td>19</td>\n",
       "      <td>37</td>\n",
       "      <td>0.003452</td>\n",
       "      <td>0.007139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ALL WORDS POSTIVE_NUMBER NEGATIVE_NUMBER  Laplas_Prob_of_P_W  \\\n",
       "0   coaster              0               2            0.000173   \n",
       "1     drunk              3               0            0.000690   \n",
       "2       bob              0               4            0.000173   \n",
       "3     cargo              1               0            0.000345   \n",
       "4   annihil             19              37            0.003452   \n",
       "\n",
       "   Laplas_Prob_of_N_W  \n",
       "0            0.000386  \n",
       "1            0.000000  \n",
       "2            0.000772  \n",
       "3            0.000000  \n",
       "4            0.007139  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B). Probablity by using simple technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. For Positive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum_posi = 0\n",
    "list_posi = []\n",
    "for j in range(0,(len(df.POSTIVE_NUMBER))):\n",
    "    temp = df.POSTIVE_NUMBER[j]\n",
    "    #sum_posi = sum_posi + temp\n",
    "    divide_posi = temp/2700\n",
    "    list_posi.append(divide_posi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = {\"posi_data\":list_posi}\n",
    "lipo = pd.DataFrame(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lipo[\"negi_data\"] = list_negi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. For Negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum_posi = 0\n",
    "list_negi = []\n",
    "for j in range(0,(len(df.NEGATIVE_NUMBER))):\n",
    "    temp = df.NEGATIVE_NUMBER[j]\n",
    "    #sum_posi = sum_posi + temp\n",
    "    divide_negi = temp/5183\n",
    "    list_negi.append(divide_negi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lipo[\"negi_data\"] = list_negi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To check the sum of a probablity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Positive lke liye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_TOTAL_SUM = 0\n",
    "for j in range(0,(len(lipo.posi_data))):\n",
    "    temp = lipo.posi_data[j]\n",
    "    P_TOTAL_SUM = P_TOTAL_SUM + temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999891"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_TOTAL_SUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Negative lke liye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOTAL_SUM = 0\n",
    "for j in range(0,(len(lipo.negi_data))):\n",
    "    temp = lipo.negi_data[j]\n",
    "    N_TOTAL_SUM = N_TOTAL_SUM + temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999813"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_TOTAL_SUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SO Sum are Nearly one means its ok, Now we will append all these probablity in dataframe (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Prob_of_P_W\"] = list_posi\n",
    "df[\"Prob_of_N_W\"] = list_negi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now split dataset into train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q = q.loc[0:799]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 5)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_q = q.loc[800:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 5)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALL WORDS</th>\n",
       "      <th>POSTIVE_NUMBER</th>\n",
       "      <th>NEGATIVE_NUMBER</th>\n",
       "      <th>Laplas_Prob_of_P_W</th>\n",
       "      <th>Laplas_Prob_of_N_W</th>\n",
       "      <th>Prob_of_P_W</th>\n",
       "      <th>Prob_of_N_W</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coaster</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drunk</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001111</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bob</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cargo</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>annihil</td>\n",
       "      <td>19</td>\n",
       "      <td>37</td>\n",
       "      <td>0.003452</td>\n",
       "      <td>0.007139</td>\n",
       "      <td>0.007037</td>\n",
       "      <td>0.007139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ALL WORDS POSTIVE_NUMBER NEGATIVE_NUMBER  Laplas_Prob_of_P_W  \\\n",
       "0   coaster              0               2            0.000173   \n",
       "1     drunk              3               0            0.000690   \n",
       "2       bob              0               4            0.000173   \n",
       "3     cargo              1               0            0.000345   \n",
       "4   annihil             19              37            0.003452   \n",
       "\n",
       "   Laplas_Prob_of_N_W  Prob_of_P_W  Prob_of_N_W  \n",
       "0            0.000386     0.000000     0.000386  \n",
       "1            0.000000     0.001111     0.000000  \n",
       "2            0.000772     0.000000     0.000772  \n",
       "3            0.000000     0.000370     0.000000  \n",
       "4            0.007139     0.007037     0.007139  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To find Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\new_conda\\lib\\site-packages\\ipykernel_launcher.py:8: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "#sum_posi = 0\n",
    "lambda_laplas_ = []\n",
    "for j in range(0,(len(df.Laplas_Prob_of_N_W))):\n",
    "    temp = df.Laplas_Prob_of_N_W[j]\n",
    "    temp1 = df.Laplas_Prob_of_P_W[j]\n",
    "    #sum_posi = sum_posi + temp\n",
    "    divide_negi = math.log(temp1/temp)\n",
    "    lambda_laplas_.append(divide_negi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Lambda_of_laplas_probablity\"] = lambda_laplas_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALL WORDS</th>\n",
       "      <th>POSTIVE_NUMBER</th>\n",
       "      <th>NEGATIVE_NUMBER</th>\n",
       "      <th>Laplas_Prob_of_P_W</th>\n",
       "      <th>Laplas_Prob_of_N_W</th>\n",
       "      <th>Prob_of_P_W</th>\n",
       "      <th>Prob_of_N_W</th>\n",
       "      <th>Lambda_of_laplas_probablity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coaster</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>-0.804586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drunk</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bob</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>-1.497733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cargo</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>annihil</td>\n",
       "      <td>19</td>\n",
       "      <td>37</td>\n",
       "      <td>0.003452</td>\n",
       "      <td>0.007139</td>\n",
       "      <td>0.007037</td>\n",
       "      <td>0.007139</td>\n",
       "      <td>-0.726624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ALL WORDS POSTIVE_NUMBER NEGATIVE_NUMBER  Laplas_Prob_of_P_W  \\\n",
       "0   coaster              0               2            0.000173   \n",
       "1     drunk              3               0            0.000690   \n",
       "2       bob              0               4            0.000173   \n",
       "3     cargo              1               0            0.000345   \n",
       "4   annihil             19              37            0.003452   \n",
       "\n",
       "   Laplas_Prob_of_N_W  Prob_of_P_W  Prob_of_N_W  Lambda_of_laplas_probablity  \n",
       "0            0.000386     0.000000     0.000386                    -0.804586  \n",
       "1            0.000000     0.001111     0.000000                          inf  \n",
       "2            0.000772     0.000000     0.000772                    -1.497733  \n",
       "3            0.000000     0.000370     0.000000                          inf  \n",
       "4            0.007139     0.007037     0.007139                    -0.726624  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Log Likelihood technique to find Positive and Negative sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = []\n",
    "cleaned_tokens = []\n",
    "list_to_load_divide_data_of_single_doc = []\n",
    "x = 1\n",
    "Lambda_Laplas = []\n",
    "\n",
    "\n",
    "for j in range (0,len(train_q.text)):\n",
    "    temp_text = train_q.text[j]\n",
    "    doc =nlp(temp_text)\n",
    "    sum_of_lambda = 0\n",
    "    for token in doc:\n",
    "        if token.lemma_ != \"-PRON-\":\n",
    "            temp = token.lemma_.lower().strip()\n",
    "            #k.append(temp)\n",
    "        else:\n",
    "            temp = token.lower_\n",
    "        if temp not in stopwords and temp not in punct:\n",
    "            series_of_all_words_column = df[\"ALL WORDS\"]\n",
    "            list_of_all_words = list(series_of_all_words_column)\n",
    "            index_of_temp = list_of_all_words.index(temp)\n",
    "            \n",
    "            #now pick the +ve and negative probablity of each token\n",
    "            lambda_of_that_word = df[\"Lambda_of_laplas_probablity\"][index_of_temp]\n",
    "            #print(token)\n",
    "            #print(lambda_of_that_word)\n",
    "            \n",
    "            #Now divide both of them for each token of a particular token\n",
    "            sum_of_lambda = sum_of_lambda + lambda_of_that_word\n",
    "            \n",
    "  \n",
    "\n",
    "    # Now define weather x is positive or negative\n",
    "    if sum_of_lambda>1:\n",
    "        k = 1\n",
    "    else:\n",
    "        k=0\n",
    "    Lambda_Laplas.append(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Output sentiment comes from loglikelihood in Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\new_conda\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "train_q[\"Log_likelihood_Sentiment\"] = Lambda_Laplas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we are converting a dataframe column into list so that we can use index function in further models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_all_words = df[\"ALL WORDS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_all_words = list(list_of_all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[\"Indexes\"]= data[\"Name\"].str.find(sub, start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model By using >>>Laplas_ Probablity<<< and >>>simple model<<<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\new_conda\\lib\\site-packages\\ipykernel_launcher.py:26: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "k = []\n",
    "cleaned_tokens = []\n",
    "list_to_load_divide_data_of_single_doc = []\n",
    "x = 1\n",
    "Laplas_new_list = []\n",
    "\n",
    "for j in range (0,len(train_q.text)):\n",
    "    temp_text = train_q.text[j]\n",
    "    doc =nlp(temp_text)\n",
    "    for token in doc:\n",
    "        if token.lemma_ != \"-PRON-\":\n",
    "            temp = token.lemma_.lower().strip()\n",
    "            #k.append(temp)\n",
    "        else:\n",
    "            temp = token.lower_\n",
    "        if temp not in stopwords and temp not in punct:\n",
    "            series_of_all_words_column = df[\"ALL WORDS\"]\n",
    "            list_of_all_words = list(series_of_all_words_column)\n",
    "            index_of_temp = list_of_all_words.index(temp)\n",
    "            \n",
    "            #now pick the +ve and negative probablity of each token\n",
    "            positive_probablity = df[\"Laplas_Prob_of_P_W\"][index_of_temp]\n",
    "            Nagetive_probablity = df[\"Laplas_Prob_of_N_W\"][index_of_temp]\n",
    "            \n",
    "            #Now divide both of them for each token of a particular token\n",
    "            divede_pos_and_Neg = positive_probablity / Nagetive_probablity\n",
    "            \n",
    "            # list_to_load_divide_data_of_single_doc\n",
    "            list_to_load_divide_data_of_single_doc.append(divede_pos_and_Neg)\n",
    "            \n",
    "    for j in range (0,len(list_to_load_divide_data_of_single_doc)):\n",
    "        x = list_to_load_divide_data_of_single_doc[j]\n",
    "        x = x*x\n",
    "    \n",
    "    # Now define weather x is positive or negative\n",
    "    if x>1:\n",
    "        k = 1\n",
    "    else:\n",
    "        k=0\n",
    "    Laplas_new_list.append(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL By using >>>Simple probablity<<< + >>>Simple model<<<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\new_conda\\lib\\site-packages\\ipykernel_launcher.py:26: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "k = []\n",
    "cleaned_tokens = []\n",
    "list_to_load_divide_data_of_single_doc = []\n",
    "x = 1\n",
    "new_list = []\n",
    "\n",
    "for j in range (0,len(train_q.text)):\n",
    "    temp_text = train_q.text[j]\n",
    "    doc =nlp(temp_text)\n",
    "    for token in doc:\n",
    "        if token.lemma_ != \"-PRON-\":\n",
    "            temp = token.lemma_.lower().strip()\n",
    "            #k.append(temp)\n",
    "        else:\n",
    "            temp = token.lower_\n",
    "        if temp not in stopwords and temp not in punct:\n",
    "            series_of_all_words_column = df[\"ALL WORDS\"]\n",
    "            list_of_all_words = list(series_of_all_words_column)\n",
    "            index_of_temp = list_of_all_words.index(temp)\n",
    "            \n",
    "            #now pick the +ve and negative probablity of each token\n",
    "            positive_probablity = df[\"Prob_of_P_W\"][index_of_temp]\n",
    "            Nagetive_probablity = df[\"Prob_of_N_W\"][index_of_temp]\n",
    "            \n",
    "            #Now divide both of them for each token of a particular token\n",
    "            divede_pos_and_Neg = positive_probablity / Nagetive_probablity\n",
    "            \n",
    "            # list_to_load_divide_data_of_single_doc\n",
    "            list_to_load_divide_data_of_single_doc.append(divede_pos_and_Neg)\n",
    "            \n",
    "    for j in range (0,len(list_to_load_divide_data_of_single_doc)):\n",
    "        x = list_to_load_divide_data_of_single_doc[j]\n",
    "        x = x*x\n",
    "    \n",
    "    # Now define weather x is positive or negative\n",
    "    if x>1:\n",
    "        k = 1\n",
    "    else:\n",
    "        k=0\n",
    "    new_list.append(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Output sentiment comes from above two model in Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\new_conda\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\hp\\Anaconda3\\new_conda\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_q[\"Sentiment\"] = new_list\n",
    "train_q[\"Laplas_Sentiment\"] = new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is a dataframe that will show the Actual sentiments and Predicted sentiments by three different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>Log_likelihood_Sentiment</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Laplas_Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deed are reason earthquak may allah forgiv us</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>resid ask shelter in place are be notifi by offic no evacu shelter in place order are expect</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>peopl receiv wildfir evacu order in california</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent photo from rubi alaska smoke from wildfir pour into school</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0   1     NaN      NaN   \n",
       "1   4     NaN      NaN   \n",
       "2   5     NaN      NaN   \n",
       "3   6     NaN      NaN   \n",
       "4   7     NaN      NaN   \n",
       "\n",
       "                                                                                           text  \\\n",
       "0                                             our deed are reason earthquak may allah forgiv us   \n",
       "1                                                          forest fire near la rong sask canada   \n",
       "2  resid ask shelter in place are be notifi by offic no evacu shelter in place order are expect   \n",
       "3                                                peopl receiv wildfir evacu order in california   \n",
       "4                      just got sent photo from rubi alaska smoke from wildfir pour into school   \n",
       "\n",
       "   target  Log_likelihood_Sentiment  Sentiment  Laplas_Sentiment  \n",
       "0       1                         1          1                 1  \n",
       "1       1                         1          1                 1  \n",
       "2       1                         1          0                 0  \n",
       "3       1                         1          1                 1  \n",
       "4       1                         1          1                 1  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_q.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1). For normal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted    0    1  All\n",
      "Actual                  \n",
      "0          431   85  516\n",
      "1           37  247  284\n",
      "All        468  332  800\n"
     ]
    }
   ],
   "source": [
    "data = {'y_Actual':   train_q.target,\n",
    "        'y_Predicted': train_q.Sentiment}\n",
    "\n",
    "df_plot = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\n",
    "\n",
    "confusion_matrix = pd.crosstab(df_plot['y_Actual'], df_plot['y_Predicted'], rownames=['Actual'], colnames=['Predicted'], margins = True)\n",
    "print (confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8475"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(train_q.target,train_q.Sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2). For Laplas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted    0    1  All\n",
      "Actual                  \n",
      "0          431   85  516\n",
      "1           37  247  284\n",
      "All        468  332  800\n"
     ]
    }
   ],
   "source": [
    "data = {'y_Actual':   train_q.target,\n",
    "        'y_Predicted': train_q.Laplas_Sentiment}\n",
    "\n",
    "df_plot = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\n",
    "\n",
    "confusion_matrix = pd.crosstab(df_plot['y_Actual'], df_plot['y_Predicted'], rownames=['Actual'], colnames=['Predicted'], margins = True)\n",
    "print (confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8475"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(train_q.target,train_q.Laplas_Sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3). For LOG LIKELIHOOD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted    0    1  All\n",
      "Actual                  \n",
      "0          493   23  516\n",
      "1           18  266  284\n",
      "All        511  289  800\n"
     ]
    }
   ],
   "source": [
    "data = {'y_Actual':   train_q.target,\n",
    "        'y_Predicted': train_q.Log_likelihood_Sentiment}\n",
    "\n",
    "df_plot = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\n",
    "\n",
    "confusion_matrix = pd.crosstab(df_plot['y_Actual'], df_plot['y_Predicted'], rownames=['Actual'], colnames=['Predicted'], margins = True)\n",
    "print (confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94875"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(train_q.target,train_q.Log_likelihood_Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Score\n",
    "\n",
    "## 1). Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.84      0.88       516\n",
      "           1       0.74      0.87      0.80       284\n",
      "\n",
      "   micro avg       0.85      0.85      0.85       800\n",
      "   macro avg       0.83      0.85      0.84       800\n",
      "weighted avg       0.86      0.85      0.85       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(train_q.target,train_q.Sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2). Loglikelihood Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       516\n",
      "           1       0.92      0.94      0.93       284\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       800\n",
      "   macro avg       0.94      0.95      0.94       800\n",
      "weighted avg       0.95      0.95      0.95       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(train_q.target,train_q.Log_likelihood_Sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Calculation of F Score for Loglikelihood  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94875"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = data[\"y_Actual\"]\n",
    "y_pred = data[\"y_Predicted\"]\n",
    "\n",
    "\n",
    "# Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall\n",
    "from sklearn.metrics import recall_score\n",
    "recall = recall_score(y_true, y_pred, average=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_true, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1 = 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9600779  0.92844677]\n"
     ]
    }
   ],
   "source": [
    "print(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute ROC curve and AUC area "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comaprision of AUC area of two different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "LOG_actual = list(train_q['target'])\n",
    "LOG_pred = list(train_q['Log_likelihood_Sentiment'])\n",
    "\n",
    "actual = list(train_q['target'])\n",
    "pred = list(train_q['Sentiment'])\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(actual, pred)\n",
    "log_fpr, log_tpr, log_threshold = roc_curve(LOG_actual, LOG_pred)\n",
    "\n",
    "\n",
    "simple_auc = auc(fpr, tpr)\n",
    "log_auc = auc(log_fpr, log_tpr)\n",
    "\n",
    "#svm_fpr, svm_tpr, threshold = roc_curve(y_test, y_pred_svm)\n",
    "#auc_svm = auc(svm_fpr, svm_tpr)\n",
    "\n",
    "plt.figure(figsize=(5, 5), dpi=100)\n",
    "#plt.plot(svm_fpr, svm_tpr, linestyle='-', label='SVM (auc = %0.3f)' % auc_svm)\n",
    "\n",
    "plt.plot(fpr, tpr, marker='.', label='Simple_auc = %0.3f' % simple_auc)\n",
    "plt.plot(log_fpr, log_tpr, marker='.', label='Log_auc = %0.3f' % log_auc)\n",
    "\n",
    "\n",
    "plt.xlabel('False Positive Rate -->')\n",
    "plt.ylabel('True Positive Rate -->')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
